{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0f10e-7a22-4b48-bc41-5a858198458f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ruptures --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b80fd-6c27-4016-bc62-ba3c18c8296e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import mlflow.statsmodels \n",
    "from typing import Optional\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ruptures as rpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7647f-17d2-47f2-98a9-1e39a0e78ed3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "input_path = \"\"\n",
    "holdout_weeks=4\n",
    "forecast_horizon=52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf759a-7039-43e1-be4a-89dd915f9576",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load Demand Data\n",
    "\n",
    "Review data, statistics and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a7555-1610-40ec-870a-34c638c0024b",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(input_path)\n",
    "    .withColumn(\"Date\", F.to_date(F.col(\"Date\"), \"M/d/yyyy\")) )\n",
    "\n",
    "stats = df.describe()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e792961-4715-4d02-b366-9858a491a7f4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Basic data cleaning and Outlier Removal for Quantity (IQR Method), grouped by Site and Fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcef3cd-a48f-4c28-b473-70d1584e3c11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, percentile_approx\n",
    "\n",
    "\n",
    "# Drop duplicate rows\n",
    "df_clean = df.dropDuplicates()\n",
    "\n",
    "# Remove rows with any nulls in key columns ('Site', 'Fuel', 'Quantity')\n",
    "df_clean = df_clean.dropna(subset=['Site', 'Fuel', 'Quantity'])\n",
    "\n",
    "\n",
    "# Calculate group-specific Q1 and Q3\n",
    "iqr_stats = df_clean.groupBy(\"Site\", \"Fuel\").agg(\n",
    "    percentile_approx(\"Quantity\", 0.25, 100).alias(\"Q1\"),\n",
    "    percentile_approx(\"Quantity\", 0.75, 100).alias(\"Q3\")\n",
    ").withColumn(\"IQR\", col(\"Q3\") - col(\"Q1\")) \\\n",
    " .withColumn(\"lower_bound\", col(\"Q1\") - 1.5 * col(\"IQR\")) \\\n",
    " .withColumn(\"upper_bound\", col(\"Q3\") + 1.5 * col(\"IQR\"))\n",
    "\n",
    "# Join bounds back to original dataframe and apply filtering\n",
    "df_iqr = df_clean.join(iqr_stats, on=[\"Site\", \"Fuel\"], how=\"left\")\n",
    "df_iqr = df_iqr.filter((col(\"Quantity\") >= col(\"lower_bound\")) & (col(\"Quantity\") <= col(\"upper_bound\")))\n",
    "\n",
    "# Remove IQR calculation columns now\n",
    "df_iqr = df_iqr.drop(\"Q1\", \"Q3\", \"IQR\", \"lower_bound\", \"upper_bound\")\n",
    "\n",
    "# Remove (Site, Fuel) groups with count < 3\n",
    "group_counts = df_iqr.groupBy(\"Site\", \"Fuel\").count()\n",
    "valid_groups = group_counts.filter(col(\"count\") >= 3).select(\"Site\", \"Fuel\")\n",
    "df_final = df_iqr.join(valid_groups, on=[\"Site\", \"Fuel\"], how=\"inner\")\n",
    "\n",
    "# Show numeric summary statistics\n",
    "stats = df_final.describe()\n",
    "display(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f3eb36-c866-4fa8-83c2-2a2f0c340a82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Clean-up, order and standardize time series for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cf2d2-6e22-4510-a6ec-40634bf45b84",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "weekly_df = (\n",
    "    df_final.withColumn(\"week_start\", F.date_trunc(\"week\", F.col(\"Date\")))\n",
    "      .groupBy(\"Site\", \"Fuel\", \"week_start\")\n",
    "      .agg(F.sum(\"Quantity\").alias(\"Quantity\"))\n",
    ")\n",
    "\n",
    "# min/max range per Site/Fuel\n",
    "ranges = (\n",
    "    weekly_df\n",
    "      .groupBy(\"Site\", \"Fuel\")\n",
    "      .agg(\n",
    "          F.min(\"week_start\").alias(\"min_week\"),\n",
    "          F.max(\"week_start\").alias(\"max_week\"),\n",
    "      )\n",
    ")\n",
    "\n",
    "# explode a complete weekly sequence (7-day step)\n",
    "calendar = (\n",
    "    ranges\n",
    "      .withColumn(\n",
    "          \"week_start\",\n",
    "          F.explode(F.sequence(F.col(\"min_week\"), F.col(\"max_week\"), F.expr(\"interval 7 days\")))\n",
    "      )\n",
    "      .select(\"Site\", \"Fuel\", \"week_start\")\n",
    ")\n",
    "\n",
    "# left join + fill missing as 0\n",
    "weekly_full = (\n",
    "    calendar\n",
    "      .join(weekly_df, on=[\"Site\", \"Fuel\", \"week_start\"], how=\"left\")\n",
    "      .withColumn(\"Quantity\", F.coalesce(F.col(\"Quantity\"), F.lit(0.0)))\n",
    ")\n",
    "\n",
    "weekly_full = (\n",
    "    weekly_full\n",
    "      .withColumn(\"Quantity\", F.col(\"Quantity\").cast(\"double\"))\n",
    "      .orderBy(\"Site\", \"Fuel\", \"week_start\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c35fc4-71e9-46be-bca1-61a8b99803d5",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "display(weekly_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372822cc-b7fa-4a9c-95a3-d938c43b5cd2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def fit_and_forecast_ets(\n",
    "    pdf: pd.DataFrame,\n",
    "    *,\n",
    "    holdout_weeks: int = 6, ## holdout weeks for backtestings\n",
    "    forecast_horizon: int = 52,\n",
    "    weekly_season_length: int = 52,\n",
    "    use_boxcox_for_ets: bool = False,\n",
    "    decay_rate: float = 0.03,\n",
    "    floor_percentile: float = 10.0,\n",
    "    min_floor: float = 10.0,\n",
    "    fallback_confidence: float = 0.60,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fit a weekly ETS model and generate historical, backtest, and future forecasts\n",
    "    for a single Site/Fuel time series.\n",
    "\n",
    "    This implementation is designed for noisy demand data with weak or inconsistent\n",
    "    seasonality. It prevents forecast collapse to zero by enforcing a data-driven\n",
    "    lower bound and produces a decaying confidence score for each forecasted point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf : pd.DataFrame\n",
    "        Input data for a single Site/Fuel combination. Must contain columns:\n",
    "        ['Site', 'Fuel', 'week_start', 'Quantity'].\n",
    "    holdout_weeks : int, default 4\n",
    "        Number of most recent weeks reserved for backtesting.\n",
    "    forecast_horizon : int, default 24\n",
    "        Number of future weeks to forecast.\n",
    "    weekly_season_length : int, default 52\n",
    "        Seasonal period (weeks). Seasonality is only enabled if at least two full\n",
    "        cycles are available.\n",
    "    use_boxcox_for_ets : bool, default False\n",
    "        Whether to apply a Box–Cox transform when fitting ETS.\n",
    "    decay_rate : float, default 0.03\n",
    "        Exponential decay rate applied to confidence scores as forecast horizon\n",
    "        increases.\n",
    "    floor_percentile : float, default 10.0\n",
    "        Percentile of the training data used to compute a lower bound for forecasts.\n",
    "    min_floor : float, default 10.0\n",
    "        Absolute minimum floor applied to forecasts (prevents hard-zero collapse).\n",
    "    fallback_confidence : float, default 0.60\n",
    "        Base confidence score used when no holdout data is available.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Concatenated DataFrame with rows of type:\n",
    "        - 'actual'   : historical training values\n",
    "        - 'backtest' : forecasts over the holdout window (if any)\n",
    "        - 'forecast' : future forecasts\n",
    "\n",
    "        Columns:\n",
    "        ['Site', 'Fuel', 'week_start', 'value', 'type', 'method', 'confidence_score']\n",
    "    \"\"\"\n",
    "\n",
    "    site, prod = map(str, (pdf[\"Site\"].iloc[0], pdf[\"Fuel\"].iloc[0]))\n",
    "\n",
    "    # --- Weekly series\n",
    "    s = pdf.copy()\n",
    "    s[\"week_start\"] = pd.to_datetime(s[\"week_start\"], errors=\"coerce\")\n",
    "    s = s.dropna(subset=[\"week_start\"]).sort_values(\"week_start\")\n",
    "\n",
    "    ts = (\n",
    "        s.set_index(\"week_start\")[\"Quantity\"]\n",
    "        .astype(float)\n",
    "        .asfreq(\"7D\")\n",
    "        .interpolate(\"linear\")\n",
    "        .bfill()\n",
    "        .ffill()\n",
    "    )\n",
    "    if len(ts) < 3 or ts.isna().any():\n",
    "        raise ValueError(f\"Insufficient clean training data for {site}-{prod}: len(ts)={len(ts)}\")\n",
    "\n",
    "    # --- Split\n",
    "    holdout_weeks = int(max(0, holdout_weeks))\n",
    "    train_end = max(3, len(ts) - holdout_weeks)\n",
    "    train, holdout = ts.iloc[:train_end], ts.iloc[train_end:]\n",
    "\n",
    "    # --- Floor to prevent hard-zero collapse\n",
    "    floor = max(min_floor, float(np.nanpercentile(train.values, floor_percentile)))\n",
    "\n",
    "    # -----------------------------\n",
    "    # Model routing + confidence\n",
    "    # -----------------------------\n",
    "    # This series family is highly sensitive to intermittency + regime switches:\n",
    "    # - mostly zeros with occasional spikes => intermittent (TSB/SBA) Teunter–Syntetos–Babai\n",
    "    # - switches between inactive/active regimes => gated hybrid\n",
    "    # - strong annual seasonality => seasonal ETS + season-aware confidence\n",
    "    EPS = 1e-6\n",
    "    CONF_MIN, CONF_MAX = 0.10, 0.95\n",
    "\n",
    "    def _smape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        denom = np.maximum(np.abs(y_true) + np.abs(y_pred), EPS)\n",
    "        return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom))  # in [0, 2]\n",
    "\n",
    "    def _wape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        denom = max(float(np.sum(np.abs(y_true))), EPS)\n",
    "        return float(np.sum(np.abs(y_pred - y_true)) / denom)\n",
    "\n",
    "    def _brier(y_true_occ: np.ndarray, p_occ: np.ndarray) -> float:\n",
    "        p = np.clip(p_occ, 0.0, 1.0)\n",
    "        return float(np.mean((p - y_true_occ) ** 2))\n",
    "\n",
    "    def _signal_strength(train_arr: np.ndarray) -> float:\n",
    "        # Simple, robust: reward non-zero rate (overall + recent), penalize extreme volatility of non-zero sizes\n",
    "        nonzero_rate = float(np.mean(train_arr > 0))\n",
    "        recent = train_arr[-13:] if len(train_arr) >= 13 else train_arr\n",
    "        recent_activity = float(np.mean(recent > 0))\n",
    "\n",
    "        nz = train_arr[train_arr > 0]\n",
    "        if len(nz) >= 5:\n",
    "            cv = float(np.std(nz) / max(np.mean(nz), EPS))\n",
    "        else:\n",
    "            cv = 2.0\n",
    "\n",
    "        score = (0.6 * nonzero_rate + 0.4 * recent_activity) * float(np.exp(-0.25 * cv))\n",
    "        return float(np.clip(score, 0.05, 1.0))\n",
    "\n",
    "    def _regime_stability(train_arr: np.ndarray) -> float:\n",
    "        # Penalize sudden activity change (classic regime switching)\n",
    "        if len(train_arr) >= 26:\n",
    "            a = float(np.mean(train_arr[-13:] > 0))\n",
    "            b = float(np.mean(train_arr[-26:-13] > 0))\n",
    "            shift = abs(a - b)\n",
    "        else:\n",
    "            shift = 0.0\n",
    "        return float(np.clip(1.0 - 0.75 * shift, 0.25, 1.0))\n",
    "\n",
    "    def _decay_conf(base: float, h: np.ndarray) -> np.ndarray:\n",
    "        return np.clip(base * np.exp(-decay_rate * h.astype(float)), CONF_MIN, CONF_MAX)\n",
    "\n",
    "    def _base_conf_from_holdout(\n",
    "        train_arr: np.ndarray,\n",
    "        hold_arr: np.ndarray,\n",
    "        pred_arr: np.ndarray,\n",
    "        *,\n",
    "        mode: str,\n",
    "        pred_occ: np.ndarray | None = None,\n",
    "    ) -> float:\n",
    "        # Composite base confidence:\n",
    "        # - fit_quality: error on holdout (metric depends on mode)\n",
    "        # - signal_strength: how much predictable signal exists\n",
    "        # - regime_stability: penalize regime transitions\n",
    "        sig = _signal_strength(train_arr)\n",
    "        stab = _regime_stability(train_arr)\n",
    "\n",
    "        if len(hold_arr) == 0:\n",
    "            base = float(np.clip(fallback_confidence, CONF_MIN, CONF_MAX))\n",
    "            return base\n",
    "\n",
    "        if mode == \"INTERMITTENT\" and pred_occ is not None:\n",
    "            # Fit_quality for intermittent demand:\n",
    "            # - size error on non-zero actuals (WAPE on non-zeros)\n",
    "            # - occurrence calibration (Brier score)\n",
    "            y_true = hold_arr.astype(float)\n",
    "            y_pred = pred_arr.astype(float)\n",
    "            y_occ = (y_true > 0).astype(float)\n",
    "            b = _brier(y_occ, pred_occ.astype(float))  # 0..1 (lower is better)\n",
    "\n",
    "            nz = y_true > 0\n",
    "            if int(nz.sum()) >= 2:\n",
    "                w = _wape(y_true[nz], y_pred[nz])\n",
    "                size_quality = float(np.exp(-2.0 * w))\n",
    "            else:\n",
    "                size_quality = float(np.exp(-1.5 * _smape(y_true, y_pred)))\n",
    "\n",
    "            occ_quality = float(np.exp(-2.0 * b))\n",
    "            fit_quality = float(np.clip(0.6 * occ_quality + 0.4 * size_quality, 0.0, 1.0))\n",
    "        else:\n",
    "            # Fit_quality for continuous/seasonal demand:\n",
    "            y_true = hold_arr.astype(float)\n",
    "            y_pred = pred_arr.astype(float)\n",
    "            nz = y_true > 0\n",
    "            if int(nz.sum()) >= 2:\n",
    "                w = _wape(y_true[nz], y_pred[nz])\n",
    "                fit_quality = float(np.exp(-2.0 * w))\n",
    "            else:\n",
    "                fit_quality = float(np.exp(-1.5 * _smape(y_true, y_pred)))\n",
    "\n",
    "        base = fit_quality * sig * stab\n",
    "        return float(np.clip(base, CONF_MIN, CONF_MAX))\n",
    "\n",
    "    def _classify(train_arr: np.ndarray) -> str:\n",
    "        zero_rate = float(np.mean(train_arr == 0))\n",
    "        recent = train_arr[-13:] if len(train_arr) >= 13 else train_arr\n",
    "        recent_nonzero = float(np.mean(recent > 0))\n",
    "        # Intermittent/lumpy \n",
    "        if zero_rate > 0.70 and recent_nonzero < 0.30:\n",
    "            return \"INTERMITTENT\"\n",
    "        # Regime switching \n",
    "        if len(train_arr) >= 26:\n",
    "            a = float(np.mean(train_arr[-13:] > 0))\n",
    "            b = float(np.mean(train_arr[-26:-13] > 0))\n",
    "            if abs(a - b) >= 0.30 and (0.30 <= zero_rate <= 0.85):\n",
    "                return \"REGIME\"\n",
    "        # Seasonal/continuous \n",
    "        return \"SEASONAL_OR_CONTINUOUS\"\n",
    "\n",
    "    # --- Intermittent demand model: TSB (preferred) + SBA fallback\n",
    "    def _fit_tsb(train_arr: np.ndarray, alpha: float = 0.20, beta: float = 0.20):\n",
    "        # TSB maintains occurrence probability p_t and demand size z_t when it occurs.\n",
    "        p = 0.10\n",
    "        z = float(np.mean(train_arr[train_arr > 0])) if np.any(train_arr > 0) else 0.0\n",
    "        for y in train_arr:\n",
    "            occ = 1.0 if y > 0 else 0.0\n",
    "            p = p + alpha * (occ - p)\n",
    "            if y > 0:\n",
    "                z = z + beta * (y - z)\n",
    "        return p, z\n",
    "\n",
    "    def _tsb_forecast(p: float, z: float, steps: int):\n",
    "        # Expected demand each step\n",
    "        yhat = np.full(shape=(steps,), fill_value=(p * z), dtype=float)\n",
    "        # Occurrence probability (constant under this simple TSB forecast)\n",
    "        phat = np.full(shape=(steps,), fill_value=p, dtype=float)\n",
    "        return yhat, phat\n",
    "\n",
    "    def _croston_sba_forecast(train_arr: np.ndarray, steps: int, alpha: float = 0.10):\n",
    "        # SBA: Croston with bias correction (1 - alpha/2)\n",
    "        nz_idx = np.where(train_arr > 0)[0]\n",
    "        if len(nz_idx) == 0:\n",
    "            return np.zeros(steps, dtype=float)\n",
    "\n",
    "        # Initialize\n",
    "        z = float(train_arr[nz_idx[0]])\n",
    "        p = float(nz_idx[0] + 1)  # interval length\n",
    "        last_idx = nz_idx[0]\n",
    "\n",
    "        for idx in nz_idx[1:]:\n",
    "            y = float(train_arr[idx])\n",
    "            interval = float(idx - last_idx)\n",
    "            z = z + alpha * (y - z)\n",
    "            p = p + alpha * (interval - p)\n",
    "            last_idx = idx\n",
    "\n",
    "        yhat = (z / max(p, EPS)) * (1.0 - alpha / 2.0)\n",
    "        return np.full(steps, yhat, dtype=float)\n",
    "\n",
    "    def mk_df(dates, values, typ, method, conf):\n",
    "        return pd.DataFrame({\n",
    "            \"Site\": site,\n",
    "            \"Fuel\": prod,\n",
    "            \"week_start\": dates,\n",
    "            \"value\": values.astype(float),\n",
    "            \"type\": typ,\n",
    "            \"method\": method,\n",
    "            \"confidence_score\": conf,\n",
    "        })\n",
    "\n",
    "    def clean(x: pd.Series | np.ndarray, idx=None, *, floor_override: float | None = None) -> pd.Series:\n",
    "        ser = pd.Series(x, index=idx)\n",
    "        lo = floor if floor_override is None else float(floor_override)\n",
    "        return ser.clip(lower=lo).round().astype(int)\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    # actuals (train)\n",
    "    parts.append(mk_df(train.index, train.values, \"actual\", \"N/A\", np.nan))\n",
    "\n",
    "    # --- Choose model family\n",
    "    train_arr = train.values.astype(float)\n",
    "    hold_arr = holdout.values.astype(float)\n",
    "    series_class = _classify(train_arr)\n",
    "\n",
    "    # --- Forecast holdout + future counts\n",
    "    h_hold, h_fut = len(holdout), int(forecast_horizon)\n",
    "    total_steps = h_hold + h_fut\n",
    "\n",
    "    # --- Default values\n",
    "    base_conf = float(np.clip(fallback_confidence, CONF_MIN, CONF_MAX))\n",
    "    method_label = \"N/A\"\n",
    "    pred_all = None\n",
    "    pred_occ_all = None\n",
    "    floor_override = None  # allow true zeros for intermittent forecasts\n",
    "\n",
    "    # -----------------------------\n",
    "    # Intermittent: TSB (do we have exogenous signals)\n",
    "    # -----------------------------\n",
    "    if series_class == \"INTERMITTENT\":\n",
    "        # NOTE: For intermittent demand, true zeros are meaningful; do not force a positive floor.\n",
    "        floor_override = 0.0\n",
    "        p, z = _fit_tsb(train_arr, alpha=0.20, beta=0.20)\n",
    "        yhat_all, phat_all = _tsb_forecast(p, z, total_steps)\n",
    "        pred_all = pd.Series(yhat_all, index=pd.RangeIndex(total_steps))\n",
    "        pred_occ_all = pd.Series(phat_all, index=pd.RangeIndex(total_steps))\n",
    "        method_label = \"TSB\"\n",
    "\n",
    "    # ------------------------------------------\n",
    "    # Regime switching: gated hybrid\n",
    "    # ------------------------------------------\n",
    "    elif series_class == \"REGIME\":\n",
    "        # Determine regime from RECENT activity in the training window\n",
    "        recent = train_arr[-13:] if len(train_arr) >= 13 else train_arr\n",
    "        recent_activity = float(np.mean(recent > 0))\n",
    "\n",
    "        if recent_activity < 0.35:\n",
    "            # Inactive regime -> intermittent\n",
    "            floor_override = 0.0\n",
    "            p, z = _fit_tsb(train_arr, alpha=0.20, beta=0.20)\n",
    "            yhat_all, phat_all = _tsb_forecast(p, z, total_steps)\n",
    "            pred_all = pd.Series(yhat_all, index=pd.RangeIndex(total_steps))\n",
    "            pred_occ_all = pd.Series(phat_all, index=pd.RangeIndex(total_steps))\n",
    "            method_label = \"HYBRID_TSB_INACTIVE\"\n",
    "        else:\n",
    "            # Active regime -> ETS (seasonal only if enough history)\n",
    "            use_seasonal = len(train) >= 2 * weekly_season_length\n",
    "            seasonal = \"add\" if use_seasonal else None\n",
    "            method_label = \"HYBRID_ETS\" if use_seasonal else \"HYBRID_ETS_NoSeasonality\"\n",
    "\n",
    "            model = ExponentialSmoothing(\n",
    "                train,\n",
    "                trend=\"add\",\n",
    "                damped_trend=True,\n",
    "                seasonal=seasonal,\n",
    "                seasonal_periods=(weekly_season_length if use_seasonal else None),\n",
    "            ).fit(optimized=True, use_boxcox=(use_boxcox_for_ets or None))\n",
    "\n",
    "            preds = model.forecast(total_steps)\n",
    "            pred_all = pd.Series(preds.values, index=pd.RangeIndex(total_steps))\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Seasonal/continuous: seasonal ETS\n",
    "    # ------------------------------------------------\n",
    "    else:\n",
    "        use_seasonal = len(train) >= 2 * weekly_season_length\n",
    "        seasonal = \"add\" if use_seasonal else None\n",
    "        method_label = \"ETS\" if use_seasonal else \"ETS_NoSeasonality\"\n",
    "\n",
    "        model = ExponentialSmoothing(\n",
    "            train,\n",
    "            trend=\"add\",\n",
    "            damped_trend=True,\n",
    "            seasonal=seasonal,\n",
    "            seasonal_periods=(weekly_season_length if use_seasonal else None),\n",
    "        ).fit(optimized=True, use_boxcox=(use_boxcox_for_ets or None))\n",
    "\n",
    "        preds = model.forecast(total_steps)\n",
    "        pred_all = pd.Series(preds.values, index=pd.RangeIndex(total_steps))\n",
    "\n",
    "    # --- Prepare indices\n",
    "    if h_hold:\n",
    "        bt_index = holdout.index\n",
    "    fut_index = pd.date_range(ts.index[-1] + pd.Timedelta(days=7), periods=h_fut, freq=\"7D\")\n",
    "\n",
    "    # --- Backtest (optional)\n",
    "    if h_hold:\n",
    "        bt_raw = pd.Series(pred_all.iloc[:h_hold].values, index=bt_index)\n",
    "\n",
    "        # Compute base confidence from holdout using the model family\n",
    "        if series_class == \"INTERMITTENT\" or (series_class == \"REGIME\" and method_label.startswith(\"HYBRID_TSB\")):\n",
    "            bt_occ = pred_occ_all.iloc[:h_hold].values if pred_occ_all is not None else None\n",
    "            base_conf = _base_conf_from_holdout(\n",
    "                train_arr=train_arr,\n",
    "                hold_arr=hold_arr,\n",
    "                pred_arr=bt_raw.values.astype(float),\n",
    "                mode=\"INTERMITTENT\",\n",
    "                pred_occ=bt_occ,\n",
    "            )\n",
    "        else:\n",
    "            base_conf = _base_conf_from_holdout(\n",
    "                train_arr=train_arr,\n",
    "                hold_arr=hold_arr,\n",
    "                pred_arr=bt_raw.values.astype(float),\n",
    "                mode=\"CONTINUOUS\",\n",
    "            )\n",
    "\n",
    "        # Continuous decay across backtest + forecast (no restart)\n",
    "        h = np.arange(1, h_hold + 1)\n",
    "        bt_conf = _decay_conf(base_conf, h)\n",
    "\n",
    "        bt = clean(bt_raw, bt_index, floor_override=floor_override)\n",
    "        parts.append(mk_df(bt.index, bt.values, \"backtest\", method_label, bt_conf.astype(float)))\n",
    "\n",
    "    # --- Forecast\n",
    "    fut_raw_vals = pred_all.iloc[h_hold:].values.astype(float)\n",
    "    fut_raw = pd.Series(fut_raw_vals, index=fut_index)\n",
    "    fut = clean(fut_raw, fut_index, floor_override=floor_override)\n",
    "\n",
    "    # Continue decay AFTER the holdout window (don't restart at h=1)\n",
    "    h = np.arange(h_hold + 1, h_hold + h_fut + 1)\n",
    "    fut_conf = _decay_conf(base_conf, h)\n",
    "\n",
    "    parts.append(mk_df(fut.index, fut.values, \"forecast\", method_label, fut_conf.astype(float)))\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74daf83-a41f-45c9-99d0-8160a7686360",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def fit_and_forecast(\n",
    "    pdf: pd.DataFrame,\n",
    "    *,\n",
    "    holdout_weeks: int = 6,\n",
    "    forecast_horizon: int = 52,\n",
    "    decay_rate: float = 0.03,\n",
    "    floor_percentile: float = 10.0,\n",
    "    min_floor: float = 10.0,\n",
    "    fallback_confidence: float = 0.60,\n",
    "    max_breakpoints: int = 5,\n",
    "    min_segment_weeks: int = 8,\n",
    "    #  knobs to prevent “runaway ramps” ---\n",
    "    min_last_segment_weeks: int = 16,  # require enough history to trust the most-recent slope\n",
    "    trend_shrink: float = 0.5,         # 0..1 shrink chosen slope toward 0\n",
    "    slope_cap: float | None = None,    # optional absolute cap on slope (units per week)\n",
    "    use_dominant_trend: bool = True,   # if False, falls back to last-segment logic\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fit a weekly piecewise-linear model with changepoints and produce:\n",
    "      - historical actuals (train)\n",
    "      - backtest predictions over the holdout window (optional)\n",
    "      - future forecasts\n",
    "\n",
    "    Key behavior (updated)\n",
    "    ----------------------\n",
    "    Changepoints are still selected by holdout MAPE, but the *forecast slope*\n",
    "    is chosen more safely:\n",
    "      - Prefer the last-segment slope only if the last segment is long enough\n",
    "        and not wildly inconsistent with the dominant historical slope.\n",
    "      - Otherwise, use a damped dominant slope or fall back to a flat (level) forecast.\n",
    "    \"\"\"\n",
    "    site, fuel = str(pdf[\"Site\"].iloc[0]), str(pdf[\"Fuel\"].iloc[0])\n",
    "\n",
    "    # Ensure a clean, regular weekly series before modeling\n",
    "    s = pdf.copy()\n",
    "    s[\"week_start\"] = pd.to_datetime(s[\"week_start\"], errors=\"coerce\")\n",
    "    s = s.dropna(subset=[\"week_start\"]).sort_values(\"week_start\")\n",
    "\n",
    "    ts = (\n",
    "        s.set_index(\"week_start\")[\"Quantity\"]\n",
    "        .astype(float)\n",
    "        .asfreq(\"7D\")\n",
    "        .interpolate(\"linear\")\n",
    "        .bfill()\n",
    "        .ffill()\n",
    "    )\n",
    "    if len(ts) < 6 or ts.isna().any():\n",
    "        raise ValueError(f\"Insufficient clean training data for {site}-{fuel}: len(ts)={len(ts)}\")\n",
    "\n",
    "    # Holdout is always taken from the most recent observations\n",
    "    holdout_weeks = int(max(0, holdout_weeks))\n",
    "    train_end = max(3, len(ts) - holdout_weeks)\n",
    "    train, holdout = ts.iloc[:train_end], ts.iloc[train_end:]\n",
    "\n",
    "    # Data-driven floor to prevent negative or collapsing extrapolation\n",
    "    floor = max(min_floor, float(np.nanpercentile(train.values, floor_percentile)))\n",
    "\n",
    "    def pack(dates, values, typ, method, conf):\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"Site\": site,\n",
    "                \"Fuel\": fuel,\n",
    "                \"week_start\": dates,\n",
    "                \"value\": np.asarray(values, float),\n",
    "                \"type\": typ,\n",
    "                \"method\": method,\n",
    "                \"confidence_score\": conf,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    y = train.values.astype(float)\n",
    "    x = np.arange(len(train), dtype=float).reshape(-1, 1)\n",
    "\n",
    "    # Detect candidate changepoints; final selection is done via holdout error\n",
    "    min_size = int(max(2, min_segment_weeks))\n",
    "    algo = rpt.Binseg(model=\"l2\", min_size=min_size).fit(y)\n",
    "\n",
    "    best_mape = np.inf\n",
    "    best_lr = None\n",
    "    best_k = 0\n",
    "    best_bkps = [len(train)]\n",
    "\n",
    "    # Cap breakpoints to feasible values to avoid ruptures.BadSegmentationParameters\n",
    "    n = len(train)\n",
    "    k_feasible_max = max(0, (n // min_size) - 1)\n",
    "    k_max = 0 if len(holdout) == 0 else min(int(max_breakpoints), k_feasible_max)\n",
    "\n",
    "    # Evaluate models with 0..K changepoints; scoring uses last-segment extrapolation\n",
    "    for k in range(0, k_max + 1):\n",
    "        try:\n",
    "            bkps = [n] if k == 0 else algo.predict(n_bkps=k)\n",
    "        except rpt.exceptions.BadSegmentationParameters:\n",
    "            continue\n",
    "\n",
    "        seg_start = 0 if len(bkps) == 1 else int(bkps[-2])\n",
    "        lr = LinearRegression().fit(x[seg_start:], y[seg_start:])\n",
    "\n",
    "        if len(holdout) == 0:\n",
    "            best_lr, best_k, best_bkps = lr, k, bkps\n",
    "            break\n",
    "\n",
    "        xh = np.arange(len(train), len(train) + len(holdout), dtype=float).reshape(-1, 1)\n",
    "        pred_h = lr.predict(xh)\n",
    "\n",
    "        denom = np.where(holdout.values != 0, np.abs(holdout.values), 1.0)\n",
    "        mape = float(np.mean(np.abs(holdout.values - pred_h) / denom) * 100.0)\n",
    "\n",
    "        if mape < best_mape:\n",
    "            best_mape, best_lr, best_k, best_bkps = mape, lr, k, bkps\n",
    "\n",
    "    # If everything failed for some reason, fall back to a global linear fit\n",
    "    if best_lr is None:\n",
    "        best_lr = LinearRegression().fit(x, y)\n",
    "        best_bkps = [n]\n",
    "        best_k = 0\n",
    "\n",
    "    # --- Choose a safe forecast slope (dominant/validated vs last-segment)\n",
    "    def _segment_slopes(bkps: list[int]) -> list[tuple[int, int, float]]:\n",
    "        segs = []\n",
    "        prev = 0\n",
    "        for b in bkps:\n",
    "            start, end = prev, int(b)\n",
    "            if end - start >= 2:\n",
    "                lr_i = LinearRegression().fit(x[start:end], y[start:end])\n",
    "                segs.append((start, end, float(lr_i.coef_[0])))\n",
    "            prev = end\n",
    "        return segs\n",
    "\n",
    "    segs = _segment_slopes(best_bkps)\n",
    "    last_start, last_end, last_slope = segs[-1] if segs else (0, n, float(best_lr.coef_[0]))\n",
    "    last_len = last_end - last_start\n",
    "\n",
    "    # Length-weighted “dominant” slope across segments\n",
    "    if len(segs) >= 2:\n",
    "        slopes = np.array([s[2] for s in segs], dtype=float)\n",
    "        lengths = np.array([s[1] - s[0] for s in segs], dtype=float)\n",
    "        dom_slope = float(np.average(slopes, weights=lengths))\n",
    "    else:\n",
    "        dom_slope = float(last_slope)\n",
    "\n",
    "    slope_eps = 1e-9\n",
    "    trust_last = (\n",
    "        (not use_dominant_trend)\n",
    "        or (\n",
    "            last_len >= int(min_last_segment_weeks)\n",
    "            and (np.sign(last_slope) == np.sign(dom_slope) or abs(dom_slope) < slope_eps)\n",
    "            and (abs(dom_slope) < slope_eps or abs(last_slope) <= 2.0 * abs(dom_slope))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if trust_last:\n",
    "        slope = float(last_slope)\n",
    "        slope_mode = \"last\"\n",
    "    else:\n",
    "        slope = 0.0 if abs(dom_slope) < slope_eps else float(dom_slope)\n",
    "        slope_mode = \"dominant\" if slope != 0.0 else \"flat\"\n",
    "\n",
    "    # Shrink and cap slope to avoid runaway ramps\n",
    "    slope = float(slope) * float(np.clip(trend_shrink, 0.0, 1.0))\n",
    "    if slope_cap is not None:\n",
    "        cap = float(abs(slope_cap))\n",
    "        slope = float(np.clip(slope, -cap, cap))\n",
    "\n",
    "    # Anchor the forecast at the last observed training point\n",
    "    y_anchor = float(train.iloc[-1])\n",
    "    t0 = float(n - 1)\n",
    "\n",
    "    method = f\"PiecewiseLinear_{best_k}CP_{slope_mode}\"\n",
    "\n",
    "    # Predict holdout + future using the selected slope\n",
    "    h_hold, h_fut = len(holdout), int(forecast_horizon)\n",
    "    steps = np.arange(1, h_hold + h_fut + 1, dtype=float)\n",
    "    preds = y_anchor + slope * steps  # y(t0 + h) = y(t0) + slope*h\n",
    "\n",
    "    parts = [pack(train.index, train.values, \"actual\", \"N/A\", np.nan)]\n",
    "    base_conf = float(fallback_confidence)\n",
    "\n",
    "    if h_hold:\n",
    "        bt_raw = preds[:h_hold]\n",
    "        denom = np.where(holdout.values != 0, np.abs(holdout.values), 1.0)\n",
    "        mape = float(np.mean(np.abs(holdout.values - bt_raw) / denom) * 100.0)\n",
    "        base_conf = float(np.clip(1.0 - mape / 100.0, 0.0, 1.0))\n",
    "\n",
    "        bt_vals = np.rint(np.maximum(bt_raw, floor)).astype(int)\n",
    "        h = np.arange(1, h_hold + 1)\n",
    "        parts.append(\n",
    "            pack(\n",
    "                holdout.index,\n",
    "                bt_vals,\n",
    "                \"backtest\",\n",
    "                method,\n",
    "                (base_conf * np.exp(-decay_rate * h)).astype(float),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fut_index = pd.date_range(ts.index[-1] + pd.Timedelta(days=7), periods=h_fut, freq=\"7D\")\n",
    "    fut_raw = preds[h_hold:]\n",
    "    fut_vals = np.rint(np.maximum(fut_raw, floor)).astype(int)\n",
    "\n",
    "    h = np.arange(1, h_fut + 1)\n",
    "    parts.append(\n",
    "        pack(\n",
    "            fut_index,\n",
    "            fut_vals,\n",
    "            \"forecast\",\n",
    "            method,\n",
    "            (base_conf * np.exp(-decay_rate * h)).astype(float),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return pd.concat(parts, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87605ca0-d889-4a15-8de9-38352e1ec54b",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "schema = T.StructType([\n",
    "    T.StructField(\"Site\", T.StringType()),\n",
    "    T.StructField(\"Fuel\", T.StringType()),\n",
    "    T.StructField(\"week_start\", T.TimestampType()),\n",
    "    T.StructField(\"value\", T.DoubleType()),     # forecast + actual\n",
    "    T.StructField(\"type\", T.StringType()),      # \"actual\" or \"forecast\"\n",
    "    T.StructField(\"method\", T.StringType()),\n",
    "    T.StructField(\"confidence_score\", T.DoubleType()),\n",
    "])\n",
    "\n",
    "result_df = (\n",
    "    weekly_full\n",
    "        .groupBy(\"Site\", \"Fuel\")\n",
    "        .applyInPandas(\n",
    "            lambda pdf: fit_and_forecast_ets(\n",
    "                pdf,\n",
    "                holdout_weeks=holdout_weeks,\n",
    "                forecast_horizon=forecast_horizon,\n",
    "            ),\n",
    "            schema=schema,\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3f6f2-22c2-4ea6-9906-00242bfe5745",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Write Forecasts\n",
    "Clean-up Past Forecasting Runs and write to `ForecastWeekly`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf9d0a8-2c14-422c-b8cc-413f87a12caa",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "sparksql",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table ForecastWeekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99a167-2366-45dc-a949-4c0b2f890464",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"ForecastWeekly\"  # your downstream table\n",
    "\n",
    "# Overwrite existing table with the new forecast\n",
    "result_df.write.format(\"delta\")\\\n",
    "                  .mode(\"overwrite\")\\\n",
    "                  .saveAsTable(table_name)\n",
    "\n",
    "print(f\" Forecasts written to Fabric data table: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f079669f-fde4-46e9-aae0-39b3fd6dea0e",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Read the 'forecastweekly' table from the Lakehouse\n",
    "df = spark.read.table(\"forecastweekly\")\n",
    "\n",
    "# Select top 1000 rows\n",
    "top_1000_df = df.limit(1000)\n",
    "\n",
    "# Display the result (if desired)\n",
    "display(top_1000_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a25ac7-f93f-4fbf-a0c6-69d71a9111ed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "| Confidence Score | Meaning                                             |\n",
    "|------------------|-----------------------------------------------------|\n",
    "| **0.85–1.00**    | Very high confidence. Model consistently accurate.  |\n",
    "| **0.65–0.85**    | Moderate confidence. Forecast is directional but has noise. |\n",
    "| **0.40–0.65**    | Low confidence. Forecast is uncertain.              |\n",
    "| **< 0.40**       | Very low confidence. Should not be used for decisions. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f847625-1ca5-448e-bbbb-7d9721425f35",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "site = \"\"\n",
    "fuel = \"\"\n",
    "\n",
    "filtered = (\n",
    "    df.filter((df.Site == site) & (df.Fuel == fuel))\n",
    "      .orderBy(\"week_start\")\n",
    ")\n",
    "\n",
    "pdf = filtered.toPandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9575873-3021-41c1-bfeb-5666a5da7b85",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Actuals\n",
    "actual = pdf[pdf[\"type\"] == \"actual\"]\n",
    "plt.plot(actual[\"week_start\"], actual[\"value\"], label=\"Actual\", marker=\"o\")\n",
    "\n",
    "# Backtest\n",
    "backtest = pdf[pdf[\"type\"] == \"backtest\"]\n",
    "plt.plot(backtest[\"week_start\"], backtest[\"value\"], label=\"Backtest\", linestyle=\"--\")\n",
    "\n",
    "# Forecast\n",
    "forecast = pdf[pdf[\"type\"] == \"forecast\"]\n",
    "plt.plot(forecast[\"week_start\"], forecast[\"value\"], label=\"Forecast\", marker=\"x\")\n",
    "\n",
    "# CONFIDENCE RIBBON\n",
    "conf = forecast[\"confidence_score\"].values        # 0 → 1\n",
    "vals = forecast[\"value\"].values.astype(float)\n",
    "\n",
    "# Convert confidence into a relative uncertainty width\n",
    "uncertainty = (1 - conf) * vals * 0.3  \n",
    "# 0.3 = scaling factor; for band width\n",
    "\n",
    "upper = vals + uncertainty\n",
    "lower = vals - uncertainty\n",
    "\n",
    "plt.fill_between(\n",
    "    forecast[\"week_start\"],\n",
    "    lower,\n",
    "    upper,\n",
    "    color=\"orange\",\n",
    "    alpha=0.20,\n",
    "    label=\"Forecast Uncertainty\"\n",
    ")\n",
    "\n",
    "plt.title(f\"Actual vs Forecast — {site} / {fuel}\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005d174b-6249-4fb7-8ef7-5b46a6f5058d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Steps to Improve Forecast Accuracy\n",
    "\n",
    "1. **Clean and prepare the data**\n",
    "   - Fix missing dates and enforce a consistent weekly frequency.\n",
    "   - Correct or remove negative or impossible values.\n",
    "   - Smooth noisy series using moving averages if appropriate.\n",
    "\n",
    "2. **Ensure enough historical data**\n",
    "   - Models perform better with longer histories.\n",
    "   - Aim for at least 1–2 years of weekly data when possible.\n",
    "\n",
    "3. **Improve seasonality detection**\n",
    "   - Use seasonal models only when seasonality is real.\n",
    "   - Tune or disable seasonality for flat or irregular series.\n",
    "\n",
    "4. **Use a more robust validation method**\n",
    "   - Increase the holdout window (e.g., 8–12 weeks instead of 4).\n",
    "   - Use multiple backtesting windows for more stable accuracy metrics.\n",
    "\n",
    "5. **Handle low-volume or erratic series separately**\n",
    "   - Use simpler models (e.g., moving average, last value).\n",
    "   - Avoid ETS on sparse or highly volatile data.\n",
    "\n",
    "6. **Remove abnormal events from training**\n",
    "   - Exclude or adjust for unusual spikes, outages, promotions, or data errors.\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "18d618ef-8a09-446a-b1ba-51bfffe27b48",
    "default_lakehouse_name": "optimizer",
    "default_lakehouse_workspace_id": "d0c9b65a-202c-4b31-8fb0-d502e8a681f8",
    "known_lakehouses": [
     {
      "id": "18d618ef-8a09-446a-b1ba-51bfffe27b48"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
