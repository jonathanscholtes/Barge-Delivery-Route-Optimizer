{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd7cd7-bd2c-49e0-a88d-bf83f3ad8c5c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "%pip install ortools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15e5fc-2c40-4133-8ae6-f4f976e60ef9",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from ortools.constraint_solver import routing_enums_pb2, pywrapcp\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Row\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import date_format\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bc7edb-6002-476e-bee9-08075df0863d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def minutes_to_datetime(week_start_date, minutes):\n",
    "    start_week = datetime.strptime(week_start_date, \"%Y-%m-%d\")\n",
    "    return start_week + timedelta(minutes=minutes)\n",
    "\n",
    "\n",
    "def to_minutes(tstr):\n",
    "    hh, mm = map(int, tstr.split(':'))\n",
    "    return hh*60 + mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbbd9c7-d4be-48f9-af75-21efbf0fe36a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def build_week_input(forecast_sdf, site_sdf, travel_sdf, week_start_date, spark):\n",
    "    \"\"\"\n",
    "    Prepare weekly input data for OR-Tools.\n",
    "    Only collects small weekly subset to driver.\n",
    "    \"\"\"\n",
    "    # Filter forecast for the week\n",
    "    week_sdf = forecast_sdf.filter(F.col(\"week_start\") == week_start_date)\n",
    "    if week_sdf.rdd.isEmpty():\n",
    "        print(f\"No forecast rows for week {week_start_date}\")\n",
    "        return None, None\n",
    "\n",
    "    # Aggregate demand per site\n",
    "    demand_sdf = (\n",
    "        week_sdf.groupBy(\"site_id\")\n",
    "        .agg(F.sum(\"forecast_units\").alias(\"forecast_units\"))\n",
    "    )\n",
    "\n",
    "    # Ensure depot exists\n",
    "    if site_sdf.filter(F.col(\"site_id\") == \"PORT0\").count() == 0:\n",
    "        depot_row = [(\"PORT0\", 0.0, 0.0, \"00:00\", \"23:59\", 0, 0)]\n",
    "        depot_sdf = spark.createDataFrame(\n",
    "            depot_row,\n",
    "            [\"site_id\",\"lat\",\"lon\",\"open_time\",\"close_time\",\"service_time_minutes\",\"max_visit_volume_units\"]\n",
    "        )\n",
    "        site_sdf = depot_sdf.unionByName(site_sdf)\n",
    "\n",
    "    # Join site specs with demand\n",
    "    df_nodes_sdf = site_sdf.join(demand_sdf, on=\"site_id\", how=\"right\").fillna(0)\n",
    "\n",
    "    # Convert to a list of Rows for OR-Tools (small data)\n",
    "    df_nodes_local = df_nodes_sdf.collect()\n",
    "\n",
    "    # Travel times as a local dict\n",
    "    travel_dict = {\n",
    "        (row[\"from_site\"], row[\"to_site\"]): float(row[\"travel_minutes\"])\n",
    "        for row in travel_sdf.collect()\n",
    "    }\n",
    "\n",
    "    return df_nodes_local, travel_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2700aa8-97ed-4511-bc4f-3589db1d350f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_data_model(df_nodes_local, travel_dict, barge_sdf):\n",
    "    \"\"\"\n",
    "    Converts local weekly data into OR-Tools input dictionary.\n",
    "    df_nodes_local: list of Row\n",
    "    \"\"\"\n",
    "    depot_id = \"PORT0\"\n",
    "    nodes = [depot_id] + [row[\"site_id\"] for row in df_nodes_local]\n",
    "\n",
    "    # Time matrix\n",
    "    n = len(nodes)\n",
    "    time_matrix = [[0]*n for _ in range(n)]\n",
    "    node_index = {node: i for i, node in enumerate(nodes)}\n",
    "\n",
    "    for i, ni in enumerate(nodes):\n",
    "        for j, nj in enumerate(nodes):\n",
    "            if i != j:\n",
    "                time_matrix[i][j] = travel_dict.get((ni, nj), 9999)\n",
    "\n",
    "    # Demands & service times\n",
    "    demands = [0] + [float(row[\"forecast_units\"]) for row in df_nodes_local]\n",
    "    service_times = [0]\n",
    "    barge_row = barge_sdf.collect()[0]  # use first barge for loading rate\n",
    "\n",
    "    def to_minutes(tstr):\n",
    "        hh, mm = map(int, tstr.split(\":\"))\n",
    "        return hh*60 + mm\n",
    "\n",
    "    windows = [(0, 24*60)]\n",
    "    for row in df_nodes_local:\n",
    "        qty = float(row[\"forecast_units\"])\n",
    "        site_min = float(row[\"service_time_minutes\"]) if row[\"service_time_minutes\"] else 30\n",
    "        service_times.append(max(site_min, math.ceil(qty / float(barge_row[\"avg_loading_rate_units_per_min\"]))))\n",
    "        windows.append((to_minutes(row[\"open_time\"]), to_minutes(row[\"close_time\"])))\n",
    "\n",
    "    # Vehicles\n",
    "    barge_rows = barge_sdf.collect()  # collect once\n",
    "    vehicle_capacities = [int(r['total_capacity_units']) for r in barge_rows]\n",
    "    vehicle_time_windows = [\n",
    "        (to_minutes(r[\"working_hours_start\"]), to_minutes(r[\"working_hours_end\"]))\n",
    "        for r in barge_sdf.collect()\n",
    "    ]\n",
    "    barge_ids = [r[\"barge_id\"] for r in barge_rows]\n",
    "    num_vehicles = len(vehicle_capacities)\n",
    "\n",
    "    return {\n",
    "        \"time_matrix\": time_matrix,\n",
    "        \"demands\": demands,\n",
    "        \"service_times\": service_times,\n",
    "        \"time_windows\": windows,\n",
    "        \"vehicle_capacities\": vehicle_capacities,\n",
    "        \"vehicle_time_windows\": vehicle_time_windows,\n",
    "        \"num_vehicles\": num_vehicles,\n",
    "        \"depot\": 0,\n",
    "        \"nodes\": nodes,\n",
    "        \"barge_ids\": barge_ids\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec359c0-29fd-4056-8c56-a20ccd79f906",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def solve_cvrptw(data, week_start_date):\n",
    "    # Initialize OR-Tools manager and routing model\n",
    "    manager = pywrapcp.RoutingIndexManager(\n",
    "        len(data['time_matrix']), \n",
    "        data['num_vehicles'], \n",
    "        data['depot']\n",
    "    )\n",
    "    routing = pywrapcp.RoutingModel(manager)\n",
    "\n",
    "    # --- Transit callback (travel + service time) ---\n",
    "    def time_callback(from_idx, to_idx):\n",
    "        from_node = manager.IndexToNode(from_idx)\n",
    "        to_node = manager.IndexToNode(to_idx)\n",
    "        return int(data['time_matrix'][from_node][to_node]) + int(data['service_times'][from_node])\n",
    "\n",
    "    transit_idx = routing.RegisterTransitCallback(time_callback)\n",
    "    routing.SetArcCostEvaluatorOfAllVehicles(transit_idx)\n",
    "\n",
    "    # --- Time dimension ---\n",
    "    horizon = 24 * 60 * 7  # one week in minutes\n",
    "    routing.AddDimension(\n",
    "        transit_idx,\n",
    "        0,          # no slack\n",
    "        horizon,    # max cumulative time\n",
    "        False,      # do not force start at zero\n",
    "        'Time'\n",
    "    )\n",
    "    time_dim = routing.GetDimensionOrDie('Time')\n",
    "\n",
    "    # Set node time windows\n",
    "    for idx, (w0, w1) in enumerate(data['time_windows']):\n",
    "        node_index = manager.NodeToIndex(idx) \n",
    "        w0 = max(0, int(round(w0)))\n",
    "        w1 = max(w0 + 1, int(round(w1)))  # ensure w1 > w0\n",
    "        time_dim.CumulVar(node_index).SetRange(w0, w1)\n",
    "\n",
    "    # --- Demand / capacity dimension ---\n",
    "    def demand_callback(from_idx):\n",
    "        return int(data['demands'][manager.IndexToNode(from_idx)])\n",
    "\n",
    "    demand_idx = routing.RegisterUnaryTransitCallback(demand_callback)\n",
    "    routing.AddDimensionWithVehicleCapacity(\n",
    "        demand_idx,\n",
    "        slack_max=0,  # slack\n",
    "        vehicle_capacities=data['vehicle_capacities'],\n",
    "        fix_start_cumul_to_zero=True,  # fix start cumulative to zero\n",
    "        name='Capacity'\n",
    "    )\n",
    "\n",
    "\n",
    "    # --- Vehicle time windows ---\n",
    "    for vid in range(data['num_vehicles']):\n",
    "        start_var = time_dim.CumulVar(routing.Start(vid))\n",
    "        end_var = time_dim.CumulVar(routing.End(vid))\n",
    "        w0, w1 = data['vehicle_time_windows'][vid]\n",
    "        start_var.SetRange(w0, w1)\n",
    "        end_var.SetRange(w0, w1)\n",
    "\n",
    "    # --- Solver parameters ---\n",
    "    params = pywrapcp.DefaultRoutingSearchParameters()\n",
    "    params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.SAVINGS\n",
    "    params.local_search_metaheuristic = routing_enums_pb2.LocalSearchMetaheuristic.GUIDED_LOCAL_SEARCH\n",
    "    params.time_limit.seconds = 60\n",
    "    params.log_search = False\n",
    "\n",
    "    # --- Solve ---\n",
    "    solution = routing.SolveWithParameters(params)\n",
    "    if not solution:\n",
    "        print(f\"No solution for week {week_start_date}\")\n",
    "        return None\n",
    "\n",
    "    # --- Extract routes ---\n",
    "    route = {barge_id: [] for barge_id in data['barge_ids']}\n",
    "    for vid in range(data['num_vehicles']):\n",
    "        idx = routing.Start(vid)\n",
    "        order = 0\n",
    "        while not routing.IsEnd(idx):\n",
    "            node = manager.IndexToNode(idx)\n",
    "            if node != data['depot']:\n",
    "                arrival = solution.Min(time_dim.CumulVar(idx))\n",
    "                departure = solution.Max(time_dim.CumulVar(idx))\n",
    "                route[data['barge_ids'][vid]].append({\n",
    "                    'order': order,\n",
    "                    'site_id': data['nodes'][node],\n",
    "                    'qty': data['demands'][node],\n",
    "                    'arrival_min': arrival,\n",
    "                    'departure_min': departure,\n",
    "                    'arrival_dt': minutes_to_datetime(week_start_date, arrival),\n",
    "                    'departure_dt': minutes_to_datetime(week_start_date, departure)\n",
    "                })\n",
    "                order += 1\n",
    "            idx = solution.Value(routing.NextVar(idx))\n",
    "\n",
    "    return route\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb7952-0365-43e3-b831-c217ef4f52d7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Read forecasts from Lakehouse table\n",
    "forecast_sdf = spark.read.table(\"forecastweekly\")\n",
    "\n",
    "# Read site specs / travel / barge from CSV or Lakehouse tables\n",
    "site_specs_sdf = spark.read.option(\"header\", True).csv(\"Files/Sites/site_specs.csv\")\n",
    "travel_sdf = spark.read.option(\"header\", True).csv(\"Files/Sites/travel_times.csv\")\n",
    "barge_sdf = spark.read.option(\"header\", True).csv(\"Files/Barges/barge_specs.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5797c3f-9677-495f-881f-cc5dbeb821ed",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Cast numeric columns\n",
    "\n",
    "sdf_map = {\n",
    "    \"forecast_sdf\": forecast_sdf,\n",
    "    \"site_specs_sdf\": site_specs_sdf,\n",
    "    \"travel_sdf\": travel_sdf,\n",
    "    \"barge_sdf\": barge_sdf\n",
    "}\n",
    "\n",
    "numeric_cols = [\"forecast_units\", \"total_capacity_units\", \"avg_loading_rate_units_per_min\", \"service_time_minutes\", \"travel_minutes\"]\n",
    "\n",
    "for name, sdf in sdf_map.items():\n",
    "    for col in numeric_cols:\n",
    "        if col in sdf.columns:\n",
    "            sdf_map[name] = sdf.withColumn(col, F.col(col).cast(\"double\"))\n",
    "    # Re-assign back to original variables\n",
    "    globals()[name] = sdf_map[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d00f440-32bb-4962-b1dd-b2c6435d2fc0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "num_weeks_to_run = 3  # Adjust as needed\n",
    "\n",
    "weeks_list = (forecast_sdf\n",
    "              .select(date_format(\"week_start\", \"yyyy-MM-dd\").alias(\"week_start\"))\n",
    "              .distinct()\n",
    "              .orderBy(\"week_start\")\n",
    "              .rdd.flatMap(lambda x: x)\n",
    "              .collect())\n",
    "weeks_list = weeks_list[:num_weeks_to_run]\n",
    "\n",
    "print(f\"Processing weeks: {weeks_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0336a-df4a-45ed-8a1a-4ddb652403df",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "def process_week(week):\n",
    "    df_nodes, travel_dict = build_week_input(forecast_sdf, site_specs_sdf, travel_sdf, week, spark)\n",
    "    if df_nodes is None:\n",
    "        return week, None\n",
    "    data_model = create_data_model(df_nodes, travel_dict, barge_sdf)\n",
    "    return week, solve_cvrptw(data_model, week)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ba4410-c8eb-4888-b376-b54621812e3e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "routes_all_weeks = {}\n",
    "total = len(weeks_list)\n",
    "\n",
    "for i, week in enumerate(weeks_list, 1):\n",
    "    print(f\"[{i}/{total}] Processing week {week}...\")\n",
    "    week, routes = process_week(week)\n",
    "    routes_all_weeks[week] = routes\n",
    "\n",
    "print(\"All weeks processed.\")\n",
    "\n",
    "#with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#    for week, routes in executor.map(process_week, weeks_list):\n",
    "#        routes_all_weeks[week] = routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9b8d76-e5ab-4f51-9d4f-52e5b848da12",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for week, week_routes in routes_all_weeks.items():\n",
    "    if not week_routes:\n",
    "        continue\n",
    "\n",
    "    rows = []\n",
    "    for barge_id, stops in week_routes.items():\n",
    "        for stop in stops:\n",
    "            rows.append(Row(\n",
    "                week_start=week,\n",
    "                barge_id=barge_id,\n",
    "                order=stop['order'],\n",
    "                site_id=stop['site_id'],\n",
    "                qty=stop['qty'],\n",
    "                arrival_min=stop['arrival_min'],\n",
    "                departure_min=stop['departure_min'],\n",
    "                arrival_dt=stop['arrival_dt'],\n",
    "                departure_dt=stop['departure_dt']\n",
    "            ))\n",
    "\n",
    "    week_sdf = spark.createDataFrame(rows)\n",
    "    week_sdf.write.format(\"delta\").mode(\"append\").saveAsTable(\"weekly_routes\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "33f5d305-a0f7-4f07-98b9-b50421f6ada9",
    "workspaceId": "d0c9b65a-202c-4b31-8fb0-d502e8a681f8"
   },
   "lakehouse": {
    "default_lakehouse": "18d618ef-8a09-446a-b1ba-51bfffe27b48",
    "default_lakehouse_name": "optimizer",
    "default_lakehouse_workspace_id": "d0c9b65a-202c-4b31-8fb0-d502e8a681f8",
    "known_lakehouses": [
     {
      "id": "18d618ef-8a09-446a-b1ba-51bfffe27b48"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
